{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итоговый проект (часть 3: бонус) #\n",
    "### Елизавета Клыкова, БКЛ181 ###\n",
    "**Описание:** программа собирает тексты по фандому \"Мистер Робот\", опубликованные на сайте https://ficbook.net и лемматизирует их. Затем на основании этих текстов и русских текстов, полученных с сайта https://archiveofourown.org, строится векторная модель, после чего модели сравниваются.\n",
    "\n",
    "Сначала включим проверку на PEP-8 и импортируем все необходимые для работы модули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "import sqlite3\n",
    "import unicodedata\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import KeyedVectors\n",
    "from html import unescape\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from string import punctuation\n",
    "from tqdm.auto import tqdm\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "morph = MorphAnalyzer()\n",
    "p = punctuation + '«»…–'\n",
    "session = requests.session()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 1. Получение данных ###\n",
    "Нам понадобятся три функции: для обхода страницы поиска; для получения данных о каждой работе на странице поиска; для получения полного текста работы со страницы отдельной работы.\n",
    "\n",
    "В качестве уникального fanfic_id будем использовать число из ссылки на полный текст работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_first_level_info(f):\n",
    "    ff = {}\n",
    "    # эти элементы не могут отсутствовать, поэтому проверка не нужна\n",
    "    ff['full_link'] = 'https://ficbook.net' + f.find(\n",
    "        'a', {'class': 'visit-link'}).attrs['href']\n",
    "    ff['fanfic_id'] = (re.search('([0-9])+', ff['full_link'])).group()\n",
    "    ff['title'] = f.find('a', {'class': 'visit-link'}).text\n",
    "    ff['author'] = f.find('span', {'class': 'author'}).text.strip()\n",
    "    ff['theme'] = f.find(\n",
    "        'span', {'class': 'direction'}).attrs['title'].split('—')[0].strip()\n",
    "    ff['description'] = f.find(\n",
    "        'div', {'class': 'fanfic-description-text'}).text.strip()\n",
    "\n",
    "    full_info = f.find('dl', {'class': 'info'}).text\n",
    "    info = re.sub(r'\\n(Фэндом:|Пэйринг |Рейтинг:|Размер:|Статус:|Метки:)',\n",
    "                  r'*\\1', full_info).split('*')\n",
    "\n",
    "    fandoms = pairings = rating = size_ = status_ = ''\n",
    "    for i in info:\n",
    "        if i.startswith('Фэндом:'):\n",
    "            fandoms = [re.sub(r'Фэндом:|\\(кроссовер\\)',\n",
    "                              '', fm).strip() for fm in i.split(',')]\n",
    "        if i.startswith('Пэйринг '):\n",
    "            pairings = [pr.replace('Пэйринг и персонажи:',\n",
    "                                   '').strip() for pr in i.split(',')]\n",
    "        if i.startswith('Рейтинг:'):\n",
    "            rating = i.replace('Рейтинг:', '').strip()\n",
    "        if i.startswith('Размер:'):\n",
    "            size_ = ', '.join([' '.join(\n",
    "                s.split())for s in i.replace('Размер:', '').split(',')])\n",
    "        if i.startswith('Статус:'):\n",
    "            status_ = i.replace('Статус:', '').strip()\n",
    "\n",
    "    if fandoms:\n",
    "        ff['fandoms'] = ', '.join(fandoms)\n",
    "    else:\n",
    "        ff['fandoms'] = fandoms\n",
    "\n",
    "    if pairings:\n",
    "        ff['pairings'] = ', '.join(pairings)\n",
    "    else:\n",
    "        ff['pairings'] = pairings\n",
    "\n",
    "    ff['rating'], ff['size_'], ff['status_'] = rating, size_, status_\n",
    "\n",
    "    # меток может не быть, поэтому проверяем; если их нет, задаем ('None')\n",
    "    try:\n",
    "        tags = (re.sub('Показать|спойлеры', '', f.find('dd', {\n",
    "            'class': 'tags'}).text.replace('Метки:', ''))).strip().split('\\n')\n",
    "    except AttributeError:\n",
    "        tags = ['None']\n",
    "    ff['tags'] = tags\n",
    "\n",
    "    # иногда авторы отключают возможность оценивания работ, поэтому проверка:\n",
    "    try:\n",
    "        ff['grade'] = f.find('span', {'class': 'value'}).text\n",
    "    except AttributeError:\n",
    "        ff['grade'] = 'Not Stated'\n",
    "\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь функция, получающая информацию со страниц отдельных работ. Отсюда нужно получить полный текст работы. Проблема заключается в том, что если в работе больше одной части, нужно получать ссылки на все части и проходить уже по ним. \n",
    "\n",
    "Проверяя, сколько частей в работе, будем получать полный текст, если часть одна, и список ссылок на все части, если больше одной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_second_level_info(ff):\n",
    "    url_one = ff['full_link']\n",
    "    page = session.get(url_one).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    # из поля \"Размер\" получим информацию о количестве частей\n",
    "    n_parts = ff['size_'].split(',')[-1].strip()\n",
    "    if n_parts == '1 часть':  # если одна часть, просто вытащим текст\n",
    "        ff['full_text'] = soup.find('div', {'id': 'content'}).text\n",
    "    else:\n",
    "        links_list = soup.find_all('ul', {'class': 'table-of-contents'})\n",
    "        links = []\n",
    "        for a_link in links_list:\n",
    "            link = 'https://ficbook.net' + a_link.find(\n",
    "                'a', {'class': 'visit-link'}).attrs['href']\n",
    "            links.append(link)\n",
    "        try:\n",
    "            ff['full_text'] = parse_third_level_info(links)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, функция, которая получает части текста со страниц глав и возвращает полный текст работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_third_level_info(links):\n",
    "    full_text = []\n",
    "    for link in links:\n",
    "        page = session.get(link).text\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        txt = soup.find('div', {'id': 'content'}).text\n",
    "        full_text.append(txt)\n",
    "    return '\\n'.join(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь объявим функцию, получающую информацию обо всех работах на странице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:80: E501 line too long (98 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "def get_nth_page(page_number):\n",
    "    url = f'https://ficbook.net/fanfiction/movies_and_tv_series/mister_robot?p={page_number}.html'\n",
    "    page = session.get(url).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    fanfics = soup.find_all('article', {'class': 'block'})\n",
    "    blocks = []\n",
    "    for f in fanfics:\n",
    "        try:\n",
    "            blocks.append(parse_first_level_info(f))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    result = []\n",
    "    for b in blocks:\n",
    "        try:\n",
    "            res = parse_second_level_info(b)\n",
    "            result.append(res)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 2. Создание базы данных ###\n",
    "Добавим в уже существующую базу данных \"eaklykova_final.db\" три таблицы:\n",
    "\n",
    "* таблицу fb_fanfics с общей информацией о работе (колонки: id, id работы, название, автор, список фандомов, список пэйрингов, рейтинг, размер, статус, направленность, количество оценок, краткое описание, ссылка на страницу работы, полный текст работы)\n",
    "* таблицу fb_tags с метками (колонки: id метки, название метки)\n",
    "* таблицу fb_fanfic_to_tag, связывающую таблицы 1 и 2 связью один-ко-многим (колонки: id, id работы, id метки)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('eaklykova_final.db')\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database():\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS fb_fanfics\n",
    "    (id INTEGER PRIMARY KEY, fanfic_id text, title text, author text,\n",
    "    fandoms text, pairings text, rating text, size_ text, status_ text,\n",
    "    theme text, grade text, description text, full_link text,\n",
    "    full_text text)\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS fb_tags\n",
    "    (id INTEGER PRIMARY KEY, tag text)\"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS fb_fanfic_to_tag\n",
    "    (id INTEGER PRIMARY KEY AUTOINCREMENT, id_fanfic int, id_tag int)\n",
    "    \"\"\")\n",
    "\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 3. Запись в базу ###\n",
    "Для каждой работы проверим, что ее уникального fanfic_id еще нет в базе. Если условие выполнено, запишем теги этой работы по одному в словарь db_tags, в котором содержатся все уникальные теги. Каждый тег сопоставляется с индексом, равным длине списка db_tags + 1. Теги с индексами записываются в таблицу tags.\n",
    "\n",
    "В свою очередь, каждой работе приписывается индекс, равный длине списка seen_fanfics + 1. Этот индекс записывается в таблицу fanfics наряду с остальной информацией (fanfic_id, название, автор и т.д.).\n",
    "\n",
    "Наконец, в таблицу связей записываются id работы и id тегов, которые ей соответствуют.\n",
    "\n",
    "В список errors записываются ошибки, возникающие при записи работ в базу. При каждом перезапуске программы туда кладется информация о работах, которые не были записаны в базу, потому что уже в ней есть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(blocks):\n",
    "    errors = []\n",
    "    for block in blocks:\n",
    "        if block['fanfic_id'] not in seen_fanfics:\n",
    "            seen_fanfics.add(block['fanfic_id'])\n",
    "            tags = []\n",
    "            for tag in block['tags']:\n",
    "                if tag in db_tags:\n",
    "                    tags.append(db_tags[tag])\n",
    "                else:\n",
    "                    db_tags[tag] = len(db_tags) + 1\n",
    "                    tags.append(db_tags[tag])\n",
    "                    cur.execute('INSERT INTO fb_tags VALUES (?, ?)', (\n",
    "                        len(db_tags), tag))\n",
    "                    con.commit()\n",
    "\n",
    "            f_id = len(seen_fanfics)\n",
    "            cur.execute('''\n",
    "            INSERT INTO fb_fanfics VALUES (?, ?, ?, ?, ?, ?, ?, ?,\n",
    "            ?, ?, ?, ?, ?, ?)''',\n",
    "                        (f_id, block['fanfic_id'], block['title'],\n",
    "                         block['author'], block['fandoms'], block['pairings'],\n",
    "                         block['rating'], block['size_'], block['status_'],\n",
    "                         block['theme'], block['grade'], block['description'],\n",
    "                         block['full_link'], block['full_text'])\n",
    "                        )\n",
    "\n",
    "            tags = [(f_id, t) for t in tags]\n",
    "            cur.executemany(\n",
    "                '''INSERT INTO fb_fanfic_to_tag (id_fanfic, id_tag)\n",
    "                VALUES (?, ?)''',\n",
    "                tags)\n",
    "            con.commit()\n",
    "\n",
    "        else:\n",
    "            errors.append('Работа с id ' + block['fanfic_id']\n",
    "                          + ' уже есть в базе')\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключаемся к базе, получаем список тегов, которые уже встретились раньше, из таблицы tags. Список уже просмотренных работ получаем из таблицы fanfics и кладем в переменную seen_fanfics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('eaklykova_final.db')\n",
    "cur = con.cursor()\n",
    "cur.execute('SELECT tag, id FROM fb_tags')\n",
    "\n",
    "db_tags = {}\n",
    "for name, idx in cur.fetchall():\n",
    "    db_tags[name] = idx\n",
    "\n",
    "cur.execute('SELECT fanfic_id FROM fb_fanfics')\n",
    "seen_fanfics = set(i[0] for i in cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем в общую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(n_pages):\n",
    "    errors = []\n",
    "    for i in tqdm(range(n_pages)):\n",
    "        try:\n",
    "            err = write_to_db(get_nth_page(i+1))\n",
    "            errors.extend(err)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим программу на всех страницах (их 17)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d1ed3e883042a38e4061cfa69de264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "errors = run_all(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 4. Создание модели ###\n",
    "#### Лемматизация, подготовка файлов ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect('eaklykova_final.db')\n",
    "cur = db.cursor()\n",
    "cur.execute(\n",
    "    '''ALTER TABLE fb_fanfics ADD COLUMN lem_texts text''')\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('SELECT fanfic_id, full_text FROM fb_fanfics')\n",
    "fb_texts = cur.fetchall()\n",
    "fb_text_list = [list(text) for text in fb_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем русские тексты с помощью NLTK и лемматизируем их с помощью pymorphy, разделив предожения переносами строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae89e6ac0ce40a6a155ff8a6a0ea624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=329), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "punct = string.punctuation  # стандартная пунктуация\n",
    "other_punct = ['``', \"\\'\\'\", '...', '--', 'https', '–',\n",
    "               '—', '«', '»', '“', '”', '’', '***', '…', '']\n",
    "for text in tqdm(fb_text_list):\n",
    "    sents = sent_tokenize(text[1].lower())\n",
    "    lemm_sents = []\n",
    "    for sent in sents:\n",
    "        tokens = word_tokenize(sent)\n",
    "        lemmas = [morph.parse(t.strip('…'))[0].normal_form\n",
    "                  for t in tokens\n",
    "                  if t not in punct and t not in other_punct]\n",
    "        if lemmas:\n",
    "            lemm_sents.append(' '.join(lemmas))\n",
    "    lem_text = '\\n'.join(lemm_sents)\n",
    "    text.append(lem_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединим тексты, полученные с сайта https://archiveofourown.org и записанные в файл 'rus_model.txt', с текстами с https://ficbook.net. Запишем все тексты в файл 'fb_model.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_file_for_model(file1, file2, texts):\n",
    "    with open(file1, encoding='utf-8') as f1:\n",
    "        rus_texts = f1.read()\n",
    "    with open(file2, 'w', encoding='utf-8') as f2:\n",
    "        all_texts = []\n",
    "        for text in texts:\n",
    "            all_texts.append(text[-1])\n",
    "        big_fb_text = ''.join(all_texts)\n",
    "        full_text = rus_texts + '\\n' + big_fb_text\n",
    "        f2.write(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_file_for_model('rus_model.txt',\n",
    "                    'fb_model.txt', fb_text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем лемматизированные тексты в базу данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm_texts_to_db(text_list):\n",
    "    for text in text_list:\n",
    "        cur.execute('''UPDATE fb_fanfics SET lem_texts = ?\n",
    "        WHERE fanfic_id = ?''', (text[-1], text[0],))\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_texts_to_db(fb_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание модели ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(filename, model_path):\n",
    "    f = filename\n",
    "    data = gensim.models.word2vec.LineSentence(f)\n",
    "    fm = gensim.models.Word2Vec(\n",
    "        data, size=300, window=5, min_count=5, iter=50)\n",
    "    fm.init_sims(replace=True)\n",
    "    fm.wv.save_word2vec_format(model_path, binary=True)\n",
    "    return len(fm.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во слов в полной русской модели: 6627\n"
     ]
    }
   ],
   "source": [
    "fb_len = create_model('fb_model.txt', 'fb_model.bin')\n",
    "print('Кол-во слов в полной русской модели:', fb_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим все три получившиеся модели и сравним их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_m = KeyedVectors.load_word2vec_format('eng_model.bin', binary=True)\n",
    "rus_m = KeyedVectors.load_word2vec_format('rus_model.bin', binary=True)\n",
    "fb_m = KeyedVectors.load_word2vec_format('fb_model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_sim_w(model, word):\n",
    "    if word in model.wv:\n",
    "        title = 'Соседи слова \"' + word + '\":\\n'\n",
    "        similar_w = model.wv.most_similar(positive=[word], topn=5)\n",
    "        sim_ws = [w[0] + ' (' + str(w[1]) + ')'\n",
    "                  for w in similar_w]\n",
    "        result = title + '\\n'.join(sim_ws)\n",
    "    else:\n",
    "        result = 'К сожалению, выбранного слова нет в модели.'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Соседи слова \"elliot\":\n",
      "tyrell (0.7937356233596802)\n",
      "he (0.7520555257797241)\n",
      "leon (0.6383670568466187)\n",
      "robot (0.5559099912643433)\n",
      "him (0.5541512966156006)\n",
      "\n",
      "Соседи слова \"эллиот\":\n",
      "он (0.7281099557876587)\n",
      "тайрелла (0.7055686116218567)\n",
      "шейла (0.6241317391395569)\n",
      "мужчина (0.5810321569442749)\n",
      "боль (0.5765253305435181)\n",
      "\n",
      "Полная модель. Соседи слова \"эллиот\":\n",
      "он (0.6010484099388123)\n",
      "тайрелла (0.47550326585769653)\n",
      "я (0.4288792908191681)\n",
      "ирвинга (0.3620915412902832)\n",
      "хакер (0.3534810543060303)\n"
     ]
    }
   ],
   "source": [
    "print(most_sim_w(eng_m, 'elliot') + '\\n')\n",
    "print(most_sim_w(rus_m, 'эллиот') + '\\n')\n",
    "print('Полная модель.', most_sim_w(fb_m, 'эллиот'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Соседи слова \"robot\":\n",
      "elliot (0.5559099912643433)\n",
      "tyrell (0.5529675483703613)\n",
      "perry (0.5231907963752747)\n",
      "leon (0.5110501050949097)\n",
      "jovonovich (0.5056238174438477)\n",
      "\n",
      "Соседи слова \"робот\":\n",
      "норма (0.6695342659950256)\n",
      "глупый (0.6450705528259277)\n",
      "i (0.6145631670951843)\n",
      "приходиться (0.6097466349601746)\n",
      "дрожать (0.5666431188583374)\n",
      "\n",
      "Полная модель. Соседи слова \"робот\":\n",
      "грэм (0.46817851066589355)\n",
      "уэлик (0.458244264125824)\n",
      "велик (0.4218754172325134)\n",
      "малёк (0.4015697240829468)\n",
      "-да (0.39449357986450195)\n"
     ]
    }
   ],
   "source": [
    "print(most_sim_w(eng_m, 'robot') + '\\n')\n",
    "print(most_sim_w(rus_m, 'робот') + '\\n')\n",
    "print('Полная модель.', most_sim_w(fb_m, 'робот'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тайрелл и разные вариации фамилии \"Уэллик\" - это один из центральных персонажей, который некоторое время работает вместе с Эллиотом и Мистером Роботом, поэтому неудивительно, что его имя появляется так часто."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Соседи слова \"corporation\":\n",
      "conglomerate (0.6791448593139648)\n",
      "government (0.5076230764389038)\n",
      "corporate (0.47649556398391724)\n",
      "capitalist (0.4698414206504822)\n",
      "academic (0.4658206105232239)\n",
      "\n",
      "Соседи слова \"корпорация\":\n",
      "абсолютно (0.8531997203826904)\n",
      "компьютерный (0.8173185586929321)\n",
      "нечего (0.8075254559516907)\n",
      "крутой (0.7634763717651367)\n",
      "неважно (0.7530659437179565)\n",
      "\n",
      "Полная модель. Соседи слова \"корпорация\":\n",
      "мировой (0.5352795124053955)\n",
      "технология (0.5063686370849609)\n",
      "технический (0.49849390983581543)\n",
      "зло (0.4869730770587921)\n",
      "правительство (0.46851664781570435)\n"
     ]
    }
   ],
   "source": [
    "print(most_sim_w(eng_m, 'corporation') + '\\n')\n",
    "print(most_sim_w(rus_m, 'корпорация') + '\\n')\n",
    "print('Полная модель.', most_sim_w(fb_m, 'корпорация'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слова, выданные первой и третьей моделями, согласуются с темой сериала - это сериал-антиутопия о борьбе группы хакеров с крупнейшей американской корпорацией E-Corp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_sim_ws(model, words):\n",
    "    word1 = words[0]\n",
    "    word2 = words[1]\n",
    "    if word1 in model.wv and word2 in model.wv:\n",
    "        title = 'Соседи слов \"' + word1 + '\" и \"' + word2 + '\":\\n'\n",
    "        similar_w = model.wv.most_similar(positive=words, topn=5)\n",
    "        sim_ws = [w[0] + ' (' + str(w[1]) + ')' for w in similar_w]\n",
    "        result = title + '\\n'.join(sim_ws)\n",
    "    else:\n",
    "        result = 'Одного или обоих слов нет в модели.'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Соседи слов \"start\" и \"end\":\n",
      "begin (0.47154802083969116)\n",
      "speed (0.39863741397857666)\n",
      "continue (0.32734882831573486)\n",
      "wake (0.29056844115257263)\n",
      "resume (0.2869288921356201)\n",
      "\n",
      "Соседи слов \"начало\" и \"конец\":\n",
      "когда-то (0.6717181205749512)\n",
      "традиция (0.6271486282348633)\n",
      "дело (0.6247795820236206)\n",
      "рассвет (0.6193835735321045)\n",
      "сей (0.6053721904754639)\n",
      "\n",
      "Полная модель. Соседи слов \"начало\" и \"конец\":\n",
      "русло (0.3601152300834656)\n",
      "начальник (0.33164262771606445)\n",
      "доходить (0.33126792311668396)\n",
      "папа (0.326962411403656)\n",
      "e-corp. (0.3231198489665985)\n"
     ]
    }
   ],
   "source": [
    "print(most_sim_ws(eng_m, ['start', 'end']) + '\\n')\n",
    "print(most_sim_ws(rus_m, ['начало', 'конец']) + '\\n')\n",
    "print('Полная модель.', most_sim_ws(fb_m, ['начало', 'конец']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Соседи слов \"good\" и \"bad\":\n",
      "great (0.5352451801300049)\n",
      "terrible (0.5308564901351929)\n",
      "amazing (0.4878973960876465)\n",
      "real (0.44867995381355286)\n",
      "stupid (0.44414809346199036)\n",
      "\n",
      "Соседи слов \"хороший\" и \"плохой\":\n",
      "факт (0.7684264183044434)\n",
      "худой (0.7443269491195679)\n",
      "чего-то (0.7266480922698975)\n",
      "объяснять (0.7126061916351318)\n",
      "попробовать (0.7089781761169434)\n",
      "\n",
      "Полная модель. Соседи слов \"хороший\" и \"плохой\":\n",
      "интересный (0.4282445013523102)\n",
      "простой (0.36124318838119507)\n",
      "когда-либо (0.3521459400653839)\n",
      "согласиться (0.34911420941352844)\n",
      "нормальный (0.3458293676376343)\n"
     ]
    }
   ],
   "source": [
    "print(most_sim_ws(eng_m, ['good', 'bad']) + '\\n')\n",
    "print(most_sim_ws(rus_m, ['хороший', 'плохой']) + '\\n')\n",
    "print('Полная модель.', most_sim_ws(fb_m, ['хороший', 'плохой']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь хорошо видно, насколько точна английская модель в противовес обеим русским. Однако более полная русская модель все же справилась немного лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Соседи слов \"memory\" и \"thought\":\n",
      "nightmare (0.43525034189224243)\n",
      "feeling (0.42609623074531555)\n",
      "emotion (0.42093342542648315)\n",
      "consciousness (0.40808865427970886)\n",
      "guilt (0.4007054567337036)\n",
      "\n",
      "Соседи слов \"память\" и \"мысль\":\n",
      "разум (0.6978389024734497)\n",
      "попкорн (0.6957900524139404)\n",
      "душа (0.6858916878700256)\n",
      "душить (0.6797796487808228)\n",
      "успокаиваться (0.675955057144165)\n",
      "\n",
      "Полная модель. Соседи слов \"память\" и \"мысль\":\n",
      "воспоминание (0.4764951765537262)\n",
      "голов (0.43954193592071533)\n",
      "прошлое (0.4044570326805115)\n",
      "пространство (0.3916492760181427)\n",
      "разум (0.3888927400112152)\n"
     ]
    }
   ],
   "source": [
    "print(most_sim_ws(eng_m, ['memory', 'thought']) + '\\n')\n",
    "print(most_sim_ws(rus_m, ['память', 'мысль']) + '\\n')\n",
    "print('Полная модель.', most_sim_ws(fb_m, ['память', 'мысль']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь превосходство более полной русской модели над неполной еще более заметно: результаты начинают напоминать слова, выданные английской моделью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_proportion(model, word1, word2, word3):\n",
    "    try:\n",
    "        comment = word1 + ' + ' + word2 + ' - ' + word3 + ' = '\n",
    "        res = model.wv.most_similar(\n",
    "            positive=[word1, word2], negative=[word3], topn=1)[0][0]\n",
    "        result = comment + res\n",
    "    except KeyError:\n",
    "        result = 'Одного или нескольких слов нет в модели.'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man + son - woman = father\n",
      "\n",
      "мужчина + сын - женщина = работа\n",
      "\n",
      "Полная модель: мужчина + сын - женщина = привязанность\n"
     ]
    }
   ],
   "source": [
    "print(semantic_proportion(eng_m, 'man', 'son', 'woman') + '\\n')\n",
    "print(semantic_proportion(rus_m, 'мужчина', 'сын', 'женщина') + '\\n')\n",
    "print('Полная модель:', semantic_proportion(\n",
    "    fb_m, 'мужчина', 'сын', 'женщина'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year + time - long = day\n",
      "\n",
      "год + время - долгий = день\n",
      "\n",
      "Полная модель: год + время - долгий = день\n"
     ]
    }
   ],
   "source": [
    "print(semantic_proportion(eng_m, 'year', 'time', 'long') + '\\n')\n",
    "print(semantic_proportion(rus_m, 'год', 'время', 'долгий') + '\\n')\n",
    "print('Полная модель:', semantic_proportion(\n",
    "    fb_m, 'год', 'время', 'долгий'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример выше - чистая случайность, но очень приятная :)\n",
    "\n",
    "**Вывод:** английская модель гораздо точнее, что объясняется бОльшим объемом текстов, использованных для ее обучения. Более полная русская модель лишь незначительно точнее, чем исходная."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
