{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итоговый проект (часть 1: краулер) #\n",
    "### Елизавета Клыкова, БКЛ181 ###\n",
    "**Описание:** программа собирает тексты работ, опубликованных на международном ресурсе https://archiveofourown.org (все работы относятся к фандому \"Mr. Robot (TV Show)\"). Тексты обрабатываются морфологическими парсерами, данные сохраняются в базу SQLite. Для русского и английского языков создаются векторные модели, которые затем сравниваются по результатам разных тестов. Английской модели можно предложить текст какой-нибудь  работы, и она заменит в нем полнозначные слова на ближайшие синонимы (реализовано только на английских текстах, т.к. там меньше морфологии). Наконец, построены вордклауды наиболее частотных полнозначных слов русского и английского языков (чтобы выяснить, насколько похожи или различаются по содержанию работы на разных языках).\n",
    "\n",
    "**Используемые инструменты:** краулеры, SQLite, морфологические парсеры, Word2vec, визуализации, dataframe.\n",
    "\n",
    "Сначала включим проверку на PEP-8 и импортируем все необходимые для работы модули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import requests\n",
    "import sqlite3\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm\n",
    "session = requests.session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 1. Получение данных (краулеры) ###\n",
    "Функция *parse_main_page* получает информацию о работах на главной странице фандома (по 20 работ на странице, сортировка по новизне). Многие элементы не являются обязательными, поэтому для их получения добавляем try-except. В качестве уникального fanfic_id будем использовать число из ссылки на полный текст работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_main_page(f):\n",
    "    ff = {}\n",
    "    # к ссылке добавляем '?view_full_work=true',\n",
    "    # чтобы не обходить главы по одной\n",
    "    ff['full_link'] = 'https://archiveofourown.org' + f.find(\n",
    "        'a').attrs['href'] + '?view_full_work=true'\n",
    "    ff['fanfic_id'] = (re.search('([0-9])+', ff['full_link'])).group()\n",
    "    ff['title'] = f.find('a').text\n",
    "\n",
    "    # для авторов с закрытым аккаунтом\n",
    "    try:\n",
    "        ff['author'] = f.find('a', {'rel': 'author'}).text\n",
    "        ff['author_link'] = 'https://archiveofourown.org' + f.find(\n",
    "            'a', {'rel': 'author'}).attrs['href']\n",
    "    except AttributeError:\n",
    "        ff['author'] = 'restricted'\n",
    "        ff['author_link'] = 'restricted'\n",
    "\n",
    "    fandoms = []\n",
    "    for fd in f.find_all('h5', {'class': 'fandoms'}):\n",
    "        fandoms.append(fd.find('a').text)\n",
    "    ff['fandoms'] = ';'.join(fandoms)\n",
    "    ff['rating'] = f.find('span', {'class': 'rating'}).attrs['title']\n",
    "    ff['category'] = f.find(\n",
    "        'span', {'class': 'category'}).attrs['title']\n",
    "    ff['status'] = f.find('span', {'class': 'iswip'}).attrs['title']\n",
    "    ff['language'] = f.find('dd', {'class': 'language'}).text\n",
    "    ff['chapters'] = f.find('dd', {'class': 'chapters'}).text\n",
    "    try:\n",
    "        ff['summary'] = f.find('blockquote').text\n",
    "    except AttributeError:\n",
    "        ff['summary'] = ''\n",
    "\n",
    "    # кол-во слов иногда не отображается, поэтому проверка\n",
    "    try:\n",
    "        words = f.find('dd', {'class': 'words'}).text\n",
    "        n_words = int(words.replace(',', ''))\n",
    "    except ValueError:\n",
    "        n_words = 0\n",
    "    ff['words'] = n_words\n",
    "\n",
    "    # предупреждения\n",
    "    try:\n",
    "        warnings = []\n",
    "        for w in f.find_all('li', {'class': 'warnings'}):\n",
    "            warning = w.find('a').text\n",
    "            warnings.append(warning)\n",
    "        ff['warnings'] = ';'.join(warnings)\n",
    "    except AttributeError:\n",
    "        ff['warnings'] = 'Not Stated'\n",
    "\n",
    "    # пэйринги\n",
    "    try:\n",
    "        pairings = []\n",
    "        for p in f.find_all('li', {'class': 'relationships'}):\n",
    "            pairing = p.find('a').text\n",
    "            pairings.append(pairing)\n",
    "        ff['pairings'] = ';'.join(pairings)\n",
    "    except AttributeError:\n",
    "        ff['pairings'] = 'Not Stated'\n",
    "\n",
    "    # персонажи\n",
    "    try:\n",
    "        characters = []\n",
    "        for c in f.find_all('li', {'class': 'characters'}):\n",
    "            character = c.find('a').text\n",
    "            characters.append(character)\n",
    "        ff['characters'] = ';'.join(characters)\n",
    "    except AttributeError:\n",
    "        ff['characters'] = 'Not Stated'\n",
    "\n",
    "    # дополнительные теги\n",
    "    try:\n",
    "        tags = []\n",
    "        for t in f.find_all('li', {'class': 'freeforms'}):\n",
    "            tag = t.find('a').text\n",
    "            tags.append(tag)\n",
    "        ff['tags'] = ';'.join(tags)\n",
    "    except AttributeError:\n",
    "        ff['tags'] = 'Not Stated'\n",
    "\n",
    "    # статистика\n",
    "    try:\n",
    "        comments = f.find('dd', {'class': 'comments'}).text\n",
    "        ff['comments'] = int(comments.replace(',', ''))\n",
    "    except (AttributeError, ValueError):\n",
    "        ff['comments'] = 0\n",
    "    try:\n",
    "        bookmarks = f.find('dd', {'class': 'bookmarks'}).text\n",
    "        ff['bookmarks'] = int(bookmarks.replace(',', ''))\n",
    "    except (AttributeError, ValueError):\n",
    "        ff['bookmarks'] = 0\n",
    "    try:\n",
    "        kudos = f.find('dd', {'class': 'kudos'}).text\n",
    "        ff['kudos'] = int(kudos.replace(',', ''))\n",
    "    except (AttributeError, ValueError):\n",
    "        ff['kudos'] = 0\n",
    "    try:\n",
    "        hits = f.find('dd', {'class': 'hits'}).text\n",
    "        ff['hits'] = int(hits.replace(',', ''))\n",
    "    except (AttributeError, ValueError):\n",
    "        ff['hits'] = 0\n",
    "\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция *parse_fanfic_page* получает полные тексты работ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fanfic_page(ff):\n",
    "    url_one = ff['full_link']\n",
    "    # некоторые работы недоступны, поэтому проверка\n",
    "    try:\n",
    "        page = session.get(url_one).text\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        # получаем даты и полный текст\n",
    "        ff['pub_date'] = soup.find('dd', {'class': 'published'}).text\n",
    "        try:\n",
    "            ff['upd_date'] = soup.find('dd', {'class': 'status'}).text\n",
    "        except AttributeError:\n",
    "            ff['upd_date'] = ff['pub_date']\n",
    "        full_text = []\n",
    "        for chapter in soup.find_all('div', {'class': 'userstuff'}):\n",
    "            for p in chapter.find_all('p'):\n",
    "                par = p.text\n",
    "                full_text.append(par)\n",
    "        ff['full_text'] = '\\n'.join(full_text)\n",
    "    except AttributeError:\n",
    "        ff['pub_date'] = 'Unknown'\n",
    "        ff['upd_date'] = 'Unknown'\n",
    "        ff['full_text'] = 'Invalid URL'\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция *get_page* получает полную информацию обо всех работах на очередной странице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:80: E501 line too long (96 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "def get_page(page_number):\n",
    "    url = f'https://archiveofourown.org/tags/Mr*d*%20Robot%20(TV)/works?page={page_number}.html'\n",
    "    page = session.get(url).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    fanfics = soup.find_all('li', {'class': 'work blurb group'})\n",
    "    blocks = []\n",
    "    for f in fanfics:\n",
    "        try:\n",
    "            blocks.append(parse_main_page(f))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    result = []\n",
    "    for b in blocks:\n",
    "        try:\n",
    "            res = parse_fanfic_page(b)\n",
    "            result.append(res)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 2. Запись в БД (SQLite) ###\n",
    "Создадим базу данных с 5 таблицами. В основной таблице fanfics хранится общая информация и тексты работ, в таблицах warnings и tags - предупреждения и теги соответственно. Таблицы fanfic_to_warning и warning_to_tag связывают работы с предупреждениями и тегами связью один-ко-многим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('eaklykova_final.db')\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database():\n",
    "    cur.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS fanfics\n",
    "    (id INTEGER PRIMARY KEY, fanfic_id text, title text, author text,\n",
    "    author_link text, fandoms text, pairings text, characters text,\n",
    "    rating text, category text, language text, status_ text, words int,\n",
    "    chapters text, pub_date text, upd_date text, summary text,\n",
    "    full_link text, full_text text, comments int, bookmarks int,\n",
    "    kudos int, hits int)\n",
    "    ''')\n",
    "\n",
    "    cur.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS warnings\n",
    "    (id INTEGER PRIMARY KEY, warning text)''')\n",
    "\n",
    "    cur.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS fanfic_to_warning\n",
    "    (id INTEGER PRIMARY KEY AUTOINCREMENT, id_fanfic int, id_warning int)\n",
    "    ''')\n",
    "\n",
    "    cur.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS tags\n",
    "    (id INTEGER PRIMARY KEY, tag text)''')\n",
    "\n",
    "    cur.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS fanfic_to_tag\n",
    "    (id INTEGER PRIMARY KEY AUTOINCREMENT, id_fanfic int, id_tag int)\n",
    "    ''')\n",
    "\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция write_to_db принимает на вход список работ и записывает информацию о них в базу данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(blocks):\n",
    "    errors = []\n",
    "    for block in blocks:\n",
    "        if block['fanfic_id'] not in seen_fanfics:\n",
    "            seen_fanfics.add(block['fanfic_id'])\n",
    "            tags = []\n",
    "            warnings = []\n",
    "            for tag in block['tags'].split(';'):\n",
    "                if tag in db_tags:\n",
    "                    tags.append(db_tags[tag])\n",
    "                else:\n",
    "                    db_tags[tag] = len(db_tags) + 1\n",
    "                    tags.append(db_tags[tag])\n",
    "                    cur.execute('INSERT INTO tags VALUES (?, ?)', (\n",
    "                        len(db_tags), tag))\n",
    "                    con.commit()\n",
    "            for warning in block['warnings'].split(';'):\n",
    "                if warning in db_warnings:\n",
    "                    warnings.append(db_warnings[warning])\n",
    "                else:\n",
    "                    db_warnings[warning] = len(db_warnings) + 1\n",
    "                    warnings.append(db_warnings[warning])\n",
    "                    cur.execute('INSERT INTO warnings VALUES (?, ?)', (\n",
    "                        len(db_warnings), warning))\n",
    "                    con.commit()\n",
    "\n",
    "            f_id = len(seen_fanfics)\n",
    "            cur.execute('''\n",
    "            INSERT INTO fanfics VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?,\n",
    "            ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',\n",
    "                        (f_id, block['fanfic_id'], block['title'],\n",
    "                         block['author'], block['author_link'],\n",
    "                         block['fandoms'], block['pairings'],\n",
    "                         block['characters'], block['rating'],\n",
    "                         block['category'], block['language'],\n",
    "                         block['status'], block['words'], block['chapters'],\n",
    "                         block['pub_date'], block['upd_date'],\n",
    "                         block['summary'], block['full_link'],\n",
    "                         block['full_text'], block['comments'],\n",
    "                         block['bookmarks'], block['kudos'], block['hits'])\n",
    "                        )\n",
    "\n",
    "            tags = [(f_id, t) for t in tags]\n",
    "            cur.executemany(\n",
    "                'INSERT INTO fanfic_to_tag (id_fanfic, id_tag) VALUES (?, ?)',\n",
    "                tags)\n",
    "            con.commit()\n",
    "\n",
    "            warnings = [(f_id, w) for w in warnings]\n",
    "            cur.executemany(\n",
    "                '''INSERT INTO fanfic_to_warning\n",
    "                (id_fanfic, id_warning) VALUES (?, ?)''',\n",
    "                warnings)\n",
    "            con.commit()\n",
    "\n",
    "        else:\n",
    "            errors.append('Работа с id ' + block['fanfic_id']\n",
    "                          + ' уже есть в базе')\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('eaklykova_final.db')\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.execute('SELECT tag, id FROM tags')\n",
    "db_tags = {}\n",
    "for name, idx in cur.fetchall():\n",
    "    db_tags[name] = idx\n",
    "\n",
    "cur.execute('SELECT warning, id FROM warnings')\n",
    "db_warnings = {}\n",
    "for name, idx in cur.fetchall():\n",
    "    db_warnings[name] = idx\n",
    "\n",
    "cur.execute('SELECT fanfic_id FROM fanfics')\n",
    "seen_fanfics = set(i[0] for i in cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция run_all получает информацию со страниц с номерами от start_n до end_n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(start_n, end_n):\n",
    "    errors = []\n",
    "    for i in tqdm(range(start_n, end_n+1)):\n",
    "        try:\n",
    "            err = write_to_db(get_page(i))\n",
    "            errors.extend(err)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В выбранном фандоме около 950 работ, размещенных на 48 страницах. Выкачаем их все. Проходим по 5 страниц за раз, затем делаем перерыв 2.5-3 минуты (иначе ругается сайт)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da57ff25d36c4900a616e4697faab0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7aabfcc26f0451fa35f9efb4d5a680f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9826fadccb64288be2e61510cc741d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4008d27b8164a71b7a78b42c5fbf91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f733e4b5ab64f01a27101c8bb48a828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab977ad1bd647baad0ec75f8f32717e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88325082b4cb43538cc935c7841c07c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c2b9ecc824418ebdc19d372599ebe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc11c6be60c449a89d24c7fc464eabbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6908178ff3a4695a89af18f06a5837e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "b = 5\n",
    "for i in range(10):\n",
    "    errors = run_all(a, b)\n",
    "    a += 5\n",
    "    # на последней итерации sleep не нужен\n",
    "    if b != 48:\n",
    "        time.sleep(random.randint(150, 180))\n",
    "    # проверяем, что не превысили число страниц\n",
    "    if b+5 > 48:\n",
    "        b = 48\n",
    "    else:\n",
    "        b += 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
